<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Research Interests</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Ying Sun</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research Interests</h1>
</div>
<p>My general research interest is in statistical signal processing, machine learning, and nonlinear optimization algorithms. My research topics include: <br /></p>
<ul>
<li><p>Statistical learning over networks;</p>
</li>
<li><p>Large-scale optimization algorithms;</p>
</li>
<li><p>Majorization minimization algorithms;</p>
</li>
<li><p>Robust covariance matrix estimation;</p>
</li>
<li><p>Sparse principal component analysis.</p>
</li>
</ul>
<h2>Distributed Learning in Heterogenous Environments</h2>
<p>The era of pervasive intelligence features a proliferation of smart devices that continuously sense, learn from, and react to dynamic environments. 
As the number of smart devices grows, tremendous amounts of data are generated in a distributed fasion and must be processed efficiently to support real-time learning and decision-making.
Designing efficient, reliable and scalable data processing algorithms is therefore of paramount importance for engineering applications nowadays. 
Achieving the goal faces challenges due to the heterogeneity arising from data, computation, and communication resources. </p>
<h3>Distributed learning under data heterogeneity</h3>
<p>TransFusion: covariate-shift robust transfer learning for high-dimensional regression<br />
Z. He, Y. Sun, J. Liu, and R. Li (AISTATS2024)<br /></p>
<p>A loopless distributed algorithm for personalized bilevel optimization<br />
Y. Niu, J. Xu, Y. Sun, Y. Huang, L. Chai (CDC2023)<br /></p>
<h3>Communication, computation, and network structure design</h3>
<p>Tackling data heterogeneity: a new unified framework for decentralized SGD with sample-induced topology<br />
Y. Huang, Y. Sun, Z. Zhu, C. Yan, and J. Xu (ICML2022)<br /></p>
<p>Hybrid local SGD for federated learning with heterogeneous communications<br />
Y. Guo, Y. Sun, R. Hui, and Y. Gong (ICLR2021)<br /></p>
<h3>Decentralized learning for structured data</h3>
<p>Implicit regularization of decentralized gradient descent for sparse regression<br />
T. Wu and Y. Sun (NeurIPS2024)</p>
<p>Distributed sparse regression via penalization <br />
Y. Ji, G. Scutari*, Y. Sun*, and H. Honnappa (JMLR2023) (*equal contribution)<br /></p>
<p>Distributed (ATC) gradient descent for high dimension sparse regression <br />
Y. Ji, G. Scutari, Y. Sun, and H. Honnappa (TIT2023)<br /></p>
<p>Decentralized dictionary learning over time-varying digraphs <br />
A. Daneshmand, Y. Sun, G. Scutari, F. Facchinei, and Brian M. Sadler (JMLR2019) <br /></p>
<h2>Efficient Algorithm Design for Statistical Signal Processing</h2>
<p>Statistics and optimization demonstrate a close interplay in data analytics. 
Sophisticated statistical models that produce high quality solutions often lead to complex highly nonconvex optimization problems. 
However, traditional optimization tools applied to these problems in theory only yield local solutions. Moreover, employing a black-box algorithm can be inefficient due to the ignorance of the problem structure and computational resources at hand. We are interested in developing problem-driven low complexity algorithms for statistical learning with provable guarantees.</p>
<h3>Majorization-minimization algorithms</h3>
<p>Majorization-minimization algorithms in signal processing, communications, and machine learning <br />
Y. Sun, P. Babu, and D. P. Palomar (TSP2017)<br /></p>
<h3>Structured covariance estimation</h3>
<p>Low-complexity algorithms for low rank clutter parameters estimation in radar systems<br />
Y. Sun, A. Breloy, P. Babu, D. P. Palomar, F. Pascal, and G. Ginolhac (TSP 2016)<br /></p>
<p>Robust estimation of structured covariance matrix for heavy-tailed elliptical distributions<br />
Y. Sun, P. Babu, and D. P. Palomar (TSP2016)<br /></p>
<p>Regularized robust estimation of mean and covariance matrix under heavy-tailed distributions<br />
Y. Sun, P. Babu, and D. P. Palomar (TSP2015)<br /></p>
<p>Regularized Tyler's scatter estimator: existence, uniqueness, and algorithms <br />
Y. Sun, P. Babu, and D. P. Palomar (TSP2014)<br /></p>
<h3>Sparse principal component analysis</h3>
<p>Orthogonal Sparse PCA and Covariance Estimation via Procrustes Reformulation<br />
K. Benidis, Y. Sun, P. Babu, D. P. Palomar (TSP2016)<br /></p>
<h2>Large-Scale Optimization Algorithms</h2>
<p>In the era of "big data", we are witnessing a fast development in data acquisition techniques. New data features such as the massive volume/dimension, heterogeneous structure, and decentralized storage challenge traditional optimization methods, most of which rely on centralized information and computation. We are interested in developing parallel and decentralized algorithms capable of solving large-scale optimization problems leveraging multiple computing units, equipped with the following desirable features:</p>
<ul>
<li><p>fast and efficient computation</p>
</li>
<li><p>flexible to problem type and network architecture </p>
</li>
<li><p>robust against delay and asynchrony. </p>
</li>
</ul>
<h3>Flexible distributed successive convex approximation</h3>
<p>Distributed optimization based on gradient-tracking revisited: enhancing convergence rate via surrogation <br /> 
Y. Sun, A. Daneshmand, and G. Scutari, (SIOPT2022).</p>
<p>Distributed big-data optimization via block-wise gradient tracking <br />
I. Notarnicola*, Y. Sun*, G. Scutari, G. Notarstefano (TAC2020) (*equal contribution)<br /></p>
<p>Distributed nonconvex constrained optimization over time-varying digraphs <br />
G. Scutari and Y. Sun (Alphabetical order) (Math. Prog. Series B, 2018)<br /></p>
<h3>Asynchronous distributed optimization</h3>
<p>Achieving linear convergence in distributed asynchronous multi-agent optimization <br />
Y. Tian, Y. Sun,  and G. Scutari (TAC2020)<br /></p>
<div id="footer">
<div id="footer-text">
Page generated 2024-11-30 17:10:09 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="research.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
